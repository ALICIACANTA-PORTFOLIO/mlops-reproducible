#!/usr/bin/env python3
"""
DiagnÃ³stico de MLflow - Verifica configuraciÃ³n y experimentos existentes.
"""

import mlflow
import pandas as pd
from pathlib import Path

def diagnose_mlflow():
    """DiagnÃ³stica el estado actual de MLflow."""
    
    print("ğŸ” DiagnÃ³stico de MLflow")
    print("=" * 50)
    
    # 1. InformaciÃ³n bÃ¡sica
    print(f"ğŸ“¦ MLflow version: {mlflow.__version__}")
    print(f"ğŸ“ Tracking URI: {mlflow.get_tracking_uri()}")
    print(f"ğŸ“‚ MLruns directory exists: {Path('mlruns').exists()}")
    
    # 2. Experimentos existentes
    print(f"\nğŸ§ª Experimentos existentes:")
    try:
        experiments = mlflow.search_experiments()
        if experiments:
            for exp in experiments:
                print(f"  - {exp.name} (ID: {exp.experiment_id})")
                
                # Contar runs en este experimento
                runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
                print(f"    Runs: {len(runs)}")
                if len(runs) > 0:
                    latest_run = runs.iloc[0]
                    print(f"    Latest run: {latest_run['run_id'][:8]}... ({latest_run['status']})")
        else:
            print("  âŒ No hay experimentos encontrados")
    except Exception as e:
        print(f"  âŒ Error al obtener experimentos: {e}")
    
    # 3. Verificar experimento especÃ­fico
    print(f"\nğŸ¯ Experimento 'obesity_classification':")
    try:
        exp = mlflow.get_experiment_by_name("obesity_classification")
        if exp:
            print(f"  âœ… Existe - ID: {exp.experiment_id}")
            runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
            print(f"  ğŸ“Š Runs totales: {len(runs)}")
            
            if len(runs) > 0:
                print(f"\nğŸ“ˆ Ãšltimos 5 runs:")
                for i, (_, run) in enumerate(runs.head().iterrows()):
                    print(f"    {i+1}. {run['run_id'][:8]}... - Status: {run['status']}")
                    if 'metrics.accuracy' in run:
                        print(f"       Accuracy: {run['metrics.accuracy']:.4f}")
        else:
            print(f"  âŒ No existe")
    except Exception as e:
        print(f"  âŒ Error: {e}")
    
    # 4. Directorio mlruns
    print(f"\nğŸ“ Contenido de mlruns/:")
    mlruns_path = Path("mlruns")
    if mlruns_path.exists():
        subdirs = [d for d in mlruns_path.iterdir() if d.is_dir()]
        print(f"  Subdirectorios: {len(subdirs)}")
        for subdir in sorted(subdirs)[:5]:  # Mostrar solo los primeros 5
            print(f"    - {subdir.name}")
    else:
        print(f"  âŒ Directorio mlruns no existe")
    
    # 5. Recomendar soluciones
    print(f"\nğŸ”§ Posibles soluciones:")
    print(f"  1. Verificar que el entrenamiento se ejecute dentro de mlflow.start_run()")
    print(f"  2. Configurar MLflow tracking URI correctamente")
    print(f"  3. Verificar permisos de escritura en directorio mlruns")
    print(f"  4. Ejecutar: mlflow ui --port 5000")

def test_simple_logging():
    """Prueba simple de logging a MLflow."""
    
    print(f"\nğŸ§ª Prueba de logging simple:")
    
    try:
        mlflow.set_experiment("test_experiment")
        
        with mlflow.start_run():
            # Log parÃ¡metros y mÃ©tricas de prueba
            mlflow.log_param("test_param", "test_value")
            mlflow.log_metric("test_metric", 0.95)
            
            run_id = mlflow.active_run().info.run_id
            print(f"  âœ… Run creado exitosamente: {run_id}")
            
        print(f"  âœ… Test de logging completado")
        
    except Exception as e:
        print(f"  âŒ Error en test de logging: {e}")

if __name__ == "__main__":
    diagnose_mlflow()
    test_simple_logging()
    
    print(f"\n" + "=" * 50)
    print(f"ğŸ’¡ Para ver la UI de MLflow, ejecuta: mlflow ui")
    print(f"ğŸŒ Luego abre: http://localhost:5000")
    print(f"=" * 50)